{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea6e1f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without GPU: 4.101707100000002\n",
      "with GPU: 1.221827900000001\n"
     ]
    }
   ],
   "source": [
    "from numba import jit, cuda\n",
    "import numpy as np\n",
    "# to measure exec time\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# normal function to run on cpu\n",
    "def func(a):\t\t\t\t\t\t\t\t\n",
    "\tfor i in range(10000000):\n",
    "\t\ta[i]+= 1\t\n",
    "\n",
    "# function optimized to run on gpu\n",
    "@jit(target_backend='cuda')\t\t\t\t\t\t\n",
    "def func2(a):\n",
    "\tfor i in range(10000000):\n",
    "\t\ta[i]+= 1\n",
    "if __name__==\"__main__\":\n",
    "\tn = 10000000\t\t\t\t\t\t\t\n",
    "\ta = np.ones(n, dtype = np.float64)\n",
    "\t\n",
    "\tstart = timer()\n",
    "\tfunc(a)\n",
    "\tprint(\"without GPU:\", timer()-start)\t\n",
    "\t\n",
    "\tstart = timer()\n",
    "\tfunc2(a)\n",
    "\tprint(\"with GPU:\", timer()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e6d7351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88048026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Quadro M2000M'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe9a1611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Quadro M2000M\n",
      "Memory Usage:\n",
      "Allocated: 0.0 GB\n",
      "Cached:    0.0 GB\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "743d0f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5563, -1.0461]])\n",
      "tensor([[ 0.3017, -0.3049]], device='cuda:0')\n",
      "tensor([[ 0.5563, -1.0461]])\n",
      "False\n",
      "tensor([[ 0.5563, -1.0461]], device='cuda:0')\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "dev = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "t1 = torch.randn(1,2)\n",
    "t2 = torch.randn(1,2).to(dev)\n",
    "print(t1)  # tensor([[-0.2678,  1.9252]])\n",
    "print(t2)  # tensor([[ 0.5117, -3.6247]], device='cuda:0')\n",
    "t1.to(dev)\n",
    "print(t1)  # tensor([[-0.2678,  1.9252]])\n",
    "print(t1.is_cuda) # False\n",
    "t1 = t1.to(dev)\n",
    "print(t1)  # tensor([[-0.2678,  1.9252]], device='cuda:0')\n",
    "print(t1.is_cuda) # True\n",
    "\n",
    "class M(nn.Module):\n",
    "    def __init__(self):        \n",
    "        super().__init__()        \n",
    "        self.l1 = nn.Linear(1,2)\n",
    "\n",
    "    def forward(self, x):                      \n",
    "        x = self.l1(x)\n",
    "        return x\n",
    "model = M()   # not on cuda\n",
    "model.to(dev) # is on cuda (all parameters)\n",
    "print(next(model.parameters()).is_cuda) # \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62ce724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "Num of GPUs available:  13\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#tf.core\n",
    "print(tf.version.VERSION)\n",
    "print(\"Num of GPUs available: \", len(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b05eaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of GPUs available:  13\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num of GPUs available: \", len(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df545bdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mConfigProto()\n\u001b[0;32m      5\u001b[0m config\u001b[38;5;241m.\u001b[39mgpu_options\u001b[38;5;241m.\u001b[39mallow_growth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m sess \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSession\u001b[49m(config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "\n",
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3364893e",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'session'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      2\u001b[0m hello \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconstant(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHello, TensorFlow!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m sess \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(sess\u001b[38;5;241m.\u001b[39mrun(hello))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'session'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "362c5745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 3584570861416968214\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 2637732251\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 17746903614946644754\n",
       " physical_device_desc: \"device: 0, name: Quadro M2000M, pci bus id: 0000:01:00.0, compute capability: 5.0\"\n",
       " xla_global_id: 416903419]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aefa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# created new environment with python  3.9\n",
    "# install numba\n",
    "#install gpu tensor flow\n",
    "#pip install tensorflow-gpu\n",
    "#conda install pytorch torchvision torchaudio cudatoolkit=11.6 -c pytorch -c conda-forge\n",
    "\n",
    "#\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0afc0699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Create some tensors\n",
    "a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "c = tf.matmul(a, b)\n",
    "\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2efef493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Place tensors on the CPU\n",
    "with tf.device('/CPU:0'):\n",
    "  a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "\n",
    "# Run on the GPU\n",
    "c = tf.matmul(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b46f8278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "try:\n",
    "  # Specify an invalid GPU device\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "    c = tf.matmul(a, b)\n",
    "    print(c)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d803de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_DEVICES = tf.config.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c316e0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LogicalDevice(name='/device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPU_DEVICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ede9654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GPU_DEVICES_NB = len(GPU_DEVICES)\n",
    "GPU_DEVICES_NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52ab4270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 GPU device(s) have been found on your notebook :\n",
      "* GPU n째0 whose name is \"/device:GPU:0\"\n",
      "\n",
      "1 CPU device(s) have been found on your notebook :\n",
      "* CPU n째0 whose name is \"/device:CPU:0\"\n"
     ]
    }
   ],
   "source": [
    "# Get the list of all logical GPU device on your notebook\n",
    "GPU_DEVICES = tf.config.list_logical_devices('GPU')\n",
    "# Get the list of all logical CPU device on your notebook\n",
    "CPU_DEVICES = tf.config.list_logical_devices('CPU')\n",
    "# Keep only the names of each GPU devices\n",
    "GPU_DEVICES_NAMES = [x.name for x in GPU_DEVICES]\n",
    "# Keep only the names of each CPU devices\n",
    "CPU_DEVICES_NAMES = [x.name for x in CPU_DEVICES]\n",
    "# The number of GPU devices\n",
    "GPU_DEVICES_NB = len(GPU_DEVICES)\n",
    "# The number of CPU devices\n",
    "CPU_DEVICES_NB = len(CPU_DEVICES)\n",
    "\n",
    "if GPU_DEVICES_NB == 0:\n",
    "    raise SystemError('No GPU device found')\n",
    "else:\n",
    "    print(f'{GPU_DEVICES_NB} GPU device(s) have been found on your notebook :')\n",
    "\n",
    "for nb in range(GPU_DEVICES_NB):\n",
    "    gpu_name = GPU_DEVICES_NAMES[nb]\n",
    "    print(f'* GPU n째{nb} whose name is \"{gpu_name}\"')\n",
    "    \n",
    "print('')\n",
    "    \n",
    "if CPU_DEVICES_NB == 0:\n",
    "    raise SystemError('No CPU device found')\n",
    "else:\n",
    "    print(f'{CPU_DEVICES_NB} CPU device(s) have been found on your notebook :')\n",
    "\n",
    "for nb in range(CPU_DEVICES_NB):\n",
    "    cpu_name = CPU_DEVICES_NAMES[nb]\n",
    "    print(f'* CPU n째{nb} whose name is \"{cpu_name}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68a0afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_multiply(vector_length):\n",
    "    vector_1 = tf.random.normal(vector_length)\n",
    "    vector_2 = tf.random.normal(vector_length)\n",
    "    return vector_1 * vector_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a37e4c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpu_operation(vector_length):\n",
    "    # If you have several GPU you can select the one to use by changing the used index of GPU_DEVICES_NAMES\n",
    "    with tf.device(GPU_DEVICES_NAMES[0]):\n",
    "        random_multiply(vector_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa39c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpu_operation(vector_length):\n",
    "    # If you have several CPU you can select the one to use by changing the used index of GPU_DEVICES_NAMES\n",
    "    with tf.device(CPU_DEVICES_NAMES[0]):\n",
    "        random_multiply(vector_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3844da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operations on vector of length 1 are 0.41479641003654943x faster on GPU than CPU\n",
      "Operations on vector of length 10 are 0.3473967426168818x faster on GPU than CPU\n",
      "Operations on vector of length 100 are 0.45070156923779914x faster on GPU than CPU\n",
      "Operations on vector of length 1000 are 0.38880606740626705x faster on GPU than CPU\n",
      "Operations on vector of length 10000 are 0.8879723196080925x faster on GPU than CPU\n",
      "Operations on vector of length 100000 are 2.354884975480433x faster on GPU than CPU\n",
      "Operations on vector of length 1000000 are 22.820617204202087x faster on GPU than CPU\n",
      "Operations on vector of length 10000000 are 231.6808221658484x faster on GPU than CPU\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
    "cpu_operation([1])\n",
    "gpu_operation([1])\n",
    "\n",
    "for i in range(8):\n",
    "    vector_length = pow(10, i)\n",
    "    cpu_time = timeit.timeit(f'cpu_operation([{vector_length}])', number=20, setup=\"from __main__ import cpu_operation\")\n",
    "    gpu_time = timeit.timeit(f'gpu_operation([{vector_length}])', number=20, setup=\"from __main__ import gpu_operation\")\n",
    "    print(f'Operations on vector of length {vector_length} are {cpu_time/gpu_time}x faster on GPU than CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a157872",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'JOB_ID'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     SUBDOMAIN \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnotebook\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m----> 8\u001b[0m     VARID \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvar-job=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mJOB_ID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      9\u001b[0m     HOST \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJOB_HOST\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m     SUBDOMAIN \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\nvidia\\lib\\os.py:679\u001b[0m, in \u001b[0;36m_Environ.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    676\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencodekey(key)]\n\u001b[0;32m    677\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;66;03m# raise KeyError with the original key value\u001b[39;00m\n\u001b[1;32m--> 679\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodevalue(value)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'JOB_ID'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if 'NOTEBOOK_ID' in os.environ:\n",
    "    VARID = \"var-notebook=\" + os.environ['NOTEBOOK_ID']\n",
    "    HOST = os.environ['NOTEBOOK_HOST']\n",
    "    SUBDOMAIN = \"notebook\"\n",
    "else:\n",
    "    VARID =  \"var-job=\" + os.environ['JOB_ID']\n",
    "    HOST = os.environ['JOB_HOST']\n",
    "    SUBDOMAIN = \"job\"\n",
    "\n",
    "\n",
    "print(f'Your resource monitoring dashboard URL is :')\n",
    "print(f'http://{HOST.replace(SUBDOMAIN, \"monitoring\")}/d/gpu/job-monitoring?orgId=1&from=now-5m&{VARID}&to=now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa69d52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
